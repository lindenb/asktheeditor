Madam, Sir,

Wikipedia has become a huge scale database storing wide-range of human knowledge, it is a promising corpus for knowledge extraction.

DBpedia (from "DB" for "database") is a project aiming to extract structured content from the information created as part of the Wikipedia project. T

Wikidata is a project of the Wikimedia Foundation: a free, collaborative, multilingual, secondary database, collecting structured data to provide support for Wikipedia, Wikimedia Commons, the other Wikimedia projects, and well beyond that.

Could you please ask the authors of your journal , once their paper has been accepted, to add  - or to update - an article in wikipedia.

References:

* Time to underpin Wikipedia wisdom. Alex Bateman	& Darren W. Logan	: Nature 468, 765 (09 December 2010) doi:10.1038/468765c 
* Wiki ware could harness the Internet for science. Kevin Yager :  Nature 440, 278 (16 March 2006) doi:10.1038/440278a
* BigData, The future of Biocuration: Nature 455, 47-50 (4 September 2008) | doi:10.1038/455047a 
* http://wiki.dbpedia.org/DBpediaLive


More than 22 million articles indexed in PubMed • Growing at about million/year and rising 
n • The production of structured data 

Nature:"

Journals should also mandate direct submission of data into appropriate databases as a part of publication. This has been implemented by the journal Plant Physiology and curators of The Arabidopsis Information Resource (TAIR) database13. On acceptance of a manuscript, the corresponding author must fill out a simple web-based form to provide appropriate genetic and molecular information about the Arabidopsis genes in the publication. The information is sent to TAIR for integration by biocurators, who work with the authors to ensure that the data reported are of high quality and accurate.

We propose three urgent actions to advance this key field. First, authors, journals and curators should immediately begin to work together to facilitate the exchange of data between journal publications and databases

Extracting, tagging with controlled vocabularies, and representing data from the literature, are some of the most important and time-consuming tasks in biocuration. Curated information from the literature serves as the gold-standard data set for computational analysis, quality assessment of high-throughput data and benchmarking of data-mining algorithms. Meanwhile, the boundaries of the biological domain that researchers study are widening rapidly, so researchers need faster and more reliable ways to understand unfamiliar domains. This too is facilitated by literature curation.

Typically, biocurators read the full text of articles and transfer the essence into a database. For a paper about the molecular biology of a particular gene, process or pathway, such information might include gene-expression patterns, mutant phenotypes, results of biochemical assays, protein-complex membership and the authors' inferences about the functions and roles of the gene products studied. As each paper uses different experimental and analysis methods, capturing this information in a consistent fashion requires intensive thought and effort. Limited resources and staff mean that most curation groups can't keep up with all the relevant literature.

How information is presented in the literature greatly affects how fast biocurators can identify and curate it. Papers still often report newly cloned genes without providing GenBank IDs or the species from which the genes were cloned. The entities discussed in a paper, including species, genes, proteins, genotypes and phenotypes must be unambiguously identified during curation. For example, using the HUGO Gene Nomenclature Committee resource (http://www.genenames.org), we find that the human gene CDKN2A has ten literature-based synonyms. One of those, p14, is also a synonym for five other genes: CDK2AP2, CTNNBL1, RPP14, S100A9 and SUB1. To confirm the identity of the gene described, curators make inferences from synonyms, reported sequences, biological context and bibliographic citations. This time-consuming and error-prone step could be eliminated by compliance with data-reporting standards4, 5, 6, 7, 8, 9.

Most recent efforts in this direction have been developed by the communities that produce large-scale genomics data. The vast majority of the peer-reviewed literature does not yet have a reporting-structure standard. As publication has become a mainly digital endeavour, however, publications and biological databases are becoming increasingly similar. Properly cross-referenced and indexed, each could serve as an access point to the other10. Such collaboration between databases and journals would improve researchers' access to data and make their work more visible.

    To date, not much of the research community is rolling up its sleeves to annotate

We recommend that all journals and reviewers require that a distinct section of the Methods (or a supplemental document) of all published articles includes approved gene symbols (which are inherently unstable) and model-organism database IDs (which do not change) for genes discussed; nucleotide or protein accession numbers (GenBank or UniProt ID) for isoforms of each gene or protein discussed; and descriptions of species, strains, cell types and genotypes used. Examples of sources for this information are listed in Table 1. This would accelerate literature curation, uphold information integrity, facilitate the proper linkage of data to other resources and support automated mining of data from papers. Another model is for authors to provide a 'structured digital abstract' — a machine-readable XML summary of pertinent facts in the article11 — along with a manuscript. This approach is in an experimental phase at the journal FEBS Letters12.
